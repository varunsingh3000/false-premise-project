{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cIMEmj9Esik",
        "outputId": "5007d564-2f78-4f2f-8469-633c468ef79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.139)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.139 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.139)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.139->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.139->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.139->boto3) (1.16.0)\n",
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: httpx<1,>=0.25 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.27.0)\n",
            "Requirement already satisfied: orjson<3.11,>=3.9.10 in /usr/local/lib/python3.10/dist-packages (from mistralai) (3.10.6)\n",
            "Requirement already satisfied: pydantic<3,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25->mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.5.2->mistralai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25->mistralai) (1.2.1)\n",
            "Collecting openai\n",
            "  Using cached openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-1.35.10\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "#@title Installing required Python packages\n",
        "!pip install boto3\n",
        "!pip install mistralai\n",
        "!pip install openai\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wW3P9qyxHHU8"
      },
      "outputs": [],
      "source": [
        "#@title Importing Python libraries and modules\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "import dateutil\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "from openai import OpenAI\n",
        "import tabulate\n",
        "import textwrap\n",
        "\n",
        "import boto3\n",
        "\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "current_date = datetime.datetime.now(\n",
        "        pytz.timezone(\"America/Los_Angeles\")\n",
        "    ).strftime(\"%B %d, %Y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "98YRRHnz0SGu"
      },
      "outputs": [],
      "source": [
        "#@title API keys\n",
        "\n",
        "\n",
        "# OpenAI's API key (sign up at https://platform.openai.com/signup to get $5 in\n",
        "# free credit that can be used during your first 3 months)\n",
        "openai_api_key = \"ADD API KEY\"  # @param {type:\"string\"}\n",
        "openai_client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        ")\n",
        "\n",
        "mistral_client = MistralClient(api_key=\"ADD API KEY\")\n",
        "\n",
        "llama2_client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1',aws_access_key_id = 'ADD API KEY',\n",
        "                          aws_secret_access_key= 'ADD API KEY')\n",
        "# SerpApi's API key (sign up at https://serpapi.com/users/sign_up?plan=free for\n",
        "# a free plan with 100 searches/month)\n",
        "serpapi_api_key = \"ADD API KEY\"  # @param {type:\"string\"}\n",
        "\n",
        "assert (\n",
        "    openai_api_key is not None and openai_api_key != \"\"\n",
        "), \"OpenAI's API key is not set\"\n",
        "assert (\n",
        "    serpapi_api_key is not None and serpapi_api_key != \"\"\n",
        "), \"SerpApi's API key is not set\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7x4S8-FHHtK1"
      },
      "outputs": [],
      "source": [
        "#@title Function calling for the base LLM\n",
        "\n",
        "\n",
        "def call_llm_gpt_api(prompt, model, temperature, max_tokens, chat_completions=True):\n",
        "  # See https://platform.openai.com/docs/guides/gpt for details\n",
        "  if chat_completions:\n",
        "    # Chat completions API\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a helpful assistant. Answer as concisely as\"\n",
        "                    f\" possible. Knowledge cutoff: {current_date}.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"What's today's date?\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"Today is {current_date} in Pacific Standard Time.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  else:\n",
        "    # Completions API\n",
        "    response = openai_client.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        prompt=prompt,\n",
        "    )\n",
        "    return response.choices[0].text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bjxRyaqz9gML"
      },
      "outputs": [],
      "source": [
        "def call_llm_mistral_api(prompt, model, temperature):\n",
        "  # See https://platform.openai.com/docs/guides/gpt for details\n",
        "\n",
        "\n",
        "  messages = f\"\"\"\n",
        "\n",
        "    \"You are a helpful assistant. Answer as concisely as possible. Knowledge cutoff: {current_date}. {prompt}\"\n",
        "   \"\"\"\n",
        "\n",
        "  message = [\n",
        "        ChatMessage(role=\"user\", content=messages)\n",
        "    ]\n",
        "\n",
        "  chat_completion = mistral_client.chat(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        messages=message\n",
        "    )\n",
        "\n",
        "  return chat_completion.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O1YU5N_dO132"
      },
      "outputs": [],
      "source": [
        "def call_llm_llama2_api(prompt, model, temperature):\n",
        "  # See https://platform.openai.com/docs/guides/gpt for details\n",
        "\n",
        "\n",
        "  messages = f\"\"\"\n",
        "\n",
        "    \"<s>[INST] <<SYS>> You are a helpful assistant. Answer as concisely as possible. Knowledge cutoff: {current_date}. {prompt}<</SYS>> [/INST]\"\n",
        "   \"\"\"\n",
        "\n",
        "  message = json.dumps({\n",
        "    \"prompt\": messages,\n",
        "    \"temperature\": temperature\n",
        "    })\n",
        "\n",
        "  accept = 'application/json'\n",
        "  contentType = 'application/json'\n",
        "\n",
        "  response = llama2_client.invoke_model(body=message, modelId=model, accept=accept, contentType=contentType)\n",
        "\n",
        "  chat_completion = json.loads(response.get(\"body\").read())\n",
        "\n",
        "  return chat_completion.get(\"generation\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1wZfMlvj0i-Z"
      },
      "outputs": [],
      "source": [
        "#@title Function calling for the search engine\n",
        "\n",
        "\n",
        "def call_search_engine(query):\n",
        "  params = {\n",
        "    \"q\": query,\n",
        "    # \"location\": \"California, United States\",\n",
        "    \"hl\": \"en\",\n",
        "    \"gl\": \"us\",\n",
        "    \"google_domain\": \"google.com\",\n",
        "    \"api_key\": serpapi_api_key,\n",
        "\n",
        "  }\n",
        "\n",
        "  search = GoogleSearch(params)\n",
        "  return search.get_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oQTRnGgcxC8h"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions for FreshPrompt\n",
        "\n",
        "\n",
        "def is_date(string, fuzzy=False):\n",
        "  # Parse a string into a date and check its validity\n",
        "  try:\n",
        "      dateutil.parser.parse(string, fuzzy=fuzzy)\n",
        "      return True\n",
        "  except ValueError:\n",
        "      return False\n",
        "\n",
        "\n",
        "def format_date(d):\n",
        "  # Standardize the date format for each search result\n",
        "  date = dateutil.parser.parse(current_date, fuzzy=True).strftime(\"%b %d, %Y\")\n",
        "  if d is None:\n",
        "    return None\n",
        "\n",
        "  for t in [\"second\", \"minute\", \"hour\"]:\n",
        "    if f\"{t} ago\" in d or f\"{t}s ago\" in d:\n",
        "      return date\n",
        "\n",
        "  t = \"day\"\n",
        "  if f\"{t} ago\" in d or f\"{t}s ago\" in d:\n",
        "    n_days = int(re.search(\"(\\d+) days? ago\", d).group(1))\n",
        "    return (\n",
        "        datetime.datetime.strptime(date, \"%b %d, %Y\")\n",
        "        - datetime.timedelta(days=n_days)\n",
        "    ).strftime(\"%b %d, %Y\")\n",
        "\n",
        "  try:\n",
        "    return dateutil.parser.parse(d, fuzzy=True).strftime(\"%b %d, %Y\")\n",
        "  except ValueError:\n",
        "    for x in d.split():\n",
        "      if is_date(x):\n",
        "        return dateutil.parser.parse(x, fuzzy=True).strftime(\"%b %d, %Y\")\n",
        "\n",
        "\n",
        "def extract_source_webpage(link):\n",
        "  # Extract source webpage\n",
        "  return (\n",
        "      link.strip()\n",
        "      .replace(\"https://www.\", \"\")\n",
        "      .replace(\"http://www.\", \"\")\n",
        "      .replace(\"https://\", \"\")\n",
        "      .replace(\"http://\", \"\")\n",
        "      .split(\"/\")[0]\n",
        "  )\n",
        "\n",
        "\n",
        "def simplify_displayed_link(displayed_link):\n",
        "  # Simplify displayed link\n",
        "  if displayed_link is None:\n",
        "    return None\n",
        "  return extract_source_webpage(displayed_link.split(' › ')[0])\n",
        "\n",
        "\n",
        "def format_search_results(search_data, title_field=None, highlight_field=None):\n",
        "  # Standardize search results as shown in Figure 3 (left) in the paper\n",
        "  field = 'snippet_highlighted_words'\n",
        "  if field in search_data and isinstance(search_data[field], list):\n",
        "    search_data[field] = ' | '.join(search_data[field])\n",
        "\n",
        "  field = 'displayed_link'\n",
        "  if field in search_data:\n",
        "    search_data[field] = simplify_displayed_link(search_data[field])\n",
        "\n",
        "  # edge case 1\n",
        "  if search_data.get('type') == 'local_time':\n",
        "    source = search_data.get('displayed_link')\n",
        "    date = format_date(search_data.get('date'))\n",
        "    title = search_data.get('title')\n",
        "\n",
        "    snippet = search_data.get('snippet')\n",
        "    if snippet is None and 'result' in search_data:\n",
        "      if 'extensions' in search_data and isinstance(\n",
        "          search_data['extensions'], list\n",
        "      ):\n",
        "        snippet = '\\n\\t'.join(\n",
        "            [search_data['result']] + search_data['extensions']\n",
        "        )\n",
        "      else:\n",
        "        snippet = search_data['result']\n",
        "\n",
        "    highlight = search_data.get('snippet_highlighted_words')\n",
        "    if highlight is None and 'result' in search_data:\n",
        "      highlight = search_data['result']\n",
        "\n",
        "  # edge case 2\n",
        "  elif 'type' in search_data and search_data['type'] == 'population_result':\n",
        "    source = search_data.get('displayed_link')\n",
        "    if source is None and 'sources' in search_data:\n",
        "      if (\n",
        "          isinstance(search_data['sources'], list)\n",
        "          and 'link' in search_data['sources'][0]\n",
        "      ):\n",
        "        source = extract_source_webpage(search_data['sources'][0]['link'])\n",
        "\n",
        "    date = format_date(search_data.get('date'))\n",
        "    if date is None and 'year' in search_data:\n",
        "      date = format_date(search_data['year'])\n",
        "\n",
        "    title = search_data.get('title')\n",
        "\n",
        "    snippet = search_data.get('snippet')\n",
        "    if snippet is None and 'population' in search_data:\n",
        "      if 'place' in search_data:\n",
        "        snippet = '\\n\\t'.join(\n",
        "            [\n",
        "                f\"{search_data['place']} / Population\",\n",
        "            ]\n",
        "            + [\n",
        "                search_data['population'],\n",
        "            ]\n",
        "        )\n",
        "      else:\n",
        "        snippet = search_data['population']\n",
        "\n",
        "    highlight = search_data.get('snippet_highlighted_words')\n",
        "    if highlight is None and 'population' in search_data:\n",
        "      highlight = search_data['population']\n",
        "\n",
        "  else:\n",
        "    source = search_data.get('displayed_link')\n",
        "    date = format_date(search_data.get('date'))\n",
        "    title = (\n",
        "        search_data.get('title')\n",
        "        if title_field is None\n",
        "        else search_data.get(title_field)\n",
        "    )\n",
        "    highlight = (\n",
        "        search_data.get('snippet_highlighted_words')\n",
        "        if highlight_field is None\n",
        "        else search_data.get(highlight_field)\n",
        "    )\n",
        "    snippet = search_data.get('snippet', '')\n",
        "\n",
        "    if 'rich_snippet' in search_data:\n",
        "      for key in ['top', 'bottom']:\n",
        "        if (\n",
        "            key in search_data['rich_snippet']\n",
        "            and 'extensions' in search_data['rich_snippet'][key]\n",
        "        ):\n",
        "          snippet = '\\n\\t'.join(\n",
        "              [snippet] + search_data['rich_snippet'][key]['extensions']\n",
        "          )\n",
        "\n",
        "    if 'list' in search_data:\n",
        "      assert isinstance(search_data['list'], list)\n",
        "      snippet = '\\n\\t'.join([snippet] + search_data['list'])\n",
        "\n",
        "    if 'contents' in search_data and 'table' in search_data['contents']:\n",
        "      tbl = search_data['contents']['table']\n",
        "      assert isinstance(tbl, list)\n",
        "      snippet += '\\n'\n",
        "      for row in tbl:\n",
        "        snippet += f'\\n{\",\".join(row)}'\n",
        "\n",
        "    if snippet is not None and snippet.strip() == '':\n",
        "      snippet = None\n",
        "\n",
        "  return {\n",
        "      'source': source,\n",
        "      'date': date,\n",
        "      'title': title,\n",
        "      'snippet': snippet,\n",
        "      'highlight': highlight,\n",
        "  }\n",
        "\n",
        "\n",
        "def format_knowledge_graph(search_data):\n",
        "  # Standardize knowledge graphs as shown in Figure 3 (left) in the paper\n",
        "  source = None\n",
        "  if \"source\" in search_data and \"link\" in search_data[\"source\"]:\n",
        "    source = extract_source_webpage(search_data[\"source\"][\"link\"])\n",
        "\n",
        "  date = None\n",
        "\n",
        "  title = None\n",
        "  if \"title\" in search_data:\n",
        "    title = search_data[\"title\"]\n",
        "    if \"type\" in search_data:\n",
        "      title += f\"\\n\\t{search_data['type']}\"\n",
        "\n",
        "  snippet = \"\"\n",
        "  for field in search_data:\n",
        "    if (\n",
        "        (field not in [\"title\", \"type\", \"kgmid\"])\n",
        "        and (\"_link\" not in field)\n",
        "        and (\"_stick\" not in field)\n",
        "        and isinstance(search_data[field], str)\n",
        "        and not search_data[field].startswith(\"http\")\n",
        "    ):\n",
        "      snippet += f\"\\n\\t{field}: {search_data[field]}\"\n",
        "\n",
        "  if snippet.strip() == \"\":\n",
        "    snippet = None\n",
        "  else:\n",
        "    snippet = snippet.strip()\n",
        "\n",
        "  highlight = None\n",
        "\n",
        "  return {\n",
        "      \"source\": source,\n",
        "      \"date\": date,\n",
        "      \"title\": title,\n",
        "      \"snippet\": snippet,\n",
        "      \"highlight\": highlight,\n",
        "  }\n",
        "\n",
        "\n",
        "def format_questions_and_answers(search_data):\n",
        "  # Standardize questions and answers as shown in Figure 3 (left) in the paper\n",
        "  source = None\n",
        "  if \"link\" in search_data:\n",
        "    source = extract_source_webpage(search_data[\"link\"])\n",
        "\n",
        "  date = None\n",
        "\n",
        "  title = None\n",
        "  if \"question\" in search_data:\n",
        "    title = search_data[\"question\"]\n",
        "\n",
        "  snippet = None\n",
        "  if \"answer\" in search_data:\n",
        "    snippet = search_data[\"answer\"]\n",
        "\n",
        "  highlight = None\n",
        "\n",
        "  return {\n",
        "      \"source\": source,\n",
        "      \"date\": date,\n",
        "      \"title\": title,\n",
        "      \"snippet\": snippet,\n",
        "      \"highlight\": highlight,\n",
        "  }\n",
        "\n",
        "\n",
        "def freshprompt_format(\n",
        "    question,\n",
        "    search_data,\n",
        "    reasoning_and_answer,\n",
        "    num_organic_results,\n",
        "    num_related_questions,\n",
        "    num_questions_and_answers,\n",
        "    num_retrieved_evidences,\n",
        "):\n",
        "  \"\"\"Build FreshPrompt for each question\n",
        "\n",
        "  Args:\n",
        "    question: The question to process.\n",
        "    search_data: Search data.\n",
        "    reasoning_and_answer: The reasoning and answer.\n",
        "    num_organic_results: Number of organic results to keep.\n",
        "    num_related_questions: Number of related questions to keep.\n",
        "    num_questions_and_answers: Number of questions and answers to keep.\n",
        "    num_retrieved_evidences: Number of retrieved evidences to keep.\n",
        "\n",
        "  Returns:\n",
        "    A prompt that incorporates retrieved evidences for each question.\n",
        "  \"\"\"\n",
        "\n",
        "  df = pd.DataFrame(columns=['source', 'date', 'title', 'snippet', 'highlight'])\n",
        "\n",
        "  # Organic results\n",
        "  organic_results = [None] * num_organic_results\n",
        "  for k in range(num_organic_results):\n",
        "    if (\n",
        "        'organic_results' in search_data\n",
        "        and len(search_data['organic_results']) > k\n",
        "    ):\n",
        "      organic_results[k] = format_search_results(\n",
        "          search_data['organic_results'][k]\n",
        "      )\n",
        "    else:\n",
        "      organic_results[k] = format_search_results({})\n",
        "\n",
        "  for d in organic_results[::-1]:\n",
        "    df = pd.concat([df, pd.DataFrame([d])], ignore_index=True)\n",
        "\n",
        "  # Related questions\n",
        "  related_questions = [None] * num_related_questions\n",
        "  for k in range(num_related_questions):\n",
        "    if (\n",
        "        'related_questions' in search_data\n",
        "        and len(search_data['related_questions']) > k\n",
        "    ):\n",
        "      related_questions[k] = format_search_results(\n",
        "          search_data['related_questions'][k], title_field='question'\n",
        "      )\n",
        "    else:\n",
        "      related_questions[k] = format_search_results({})\n",
        "\n",
        "  for d in related_questions[::-1]:\n",
        "    df = pd.concat([df, pd.DataFrame([d])], ignore_index=True)\n",
        "\n",
        "  # Questions and Answers\n",
        "  questions_and_answers = [None] * num_questions_and_answers\n",
        "  for k in range(num_questions_and_answers):\n",
        "    if (\n",
        "        'questions_and_answers' in search_data\n",
        "        and len(search_data['questions_and_answers']) > k\n",
        "    ):\n",
        "      questions_and_answers[k] = format_questions_and_answers(\n",
        "          search_data['questions_and_answers'][k]\n",
        "      )\n",
        "    else:\n",
        "      questions_and_answers[k] = format_questions_and_answers({})\n",
        "\n",
        "  for d in questions_and_answers[::-1]:\n",
        "    df = pd.concat([df, pd.DataFrame([d])], ignore_index=True)\n",
        "\n",
        "  # Knowledge graph\n",
        "  knowledge_graph = None\n",
        "  if 'knowledge_graph' in search_data:\n",
        "    knowledge_graph = format_knowledge_graph(search_data['knowledge_graph'])\n",
        "  else:\n",
        "    knowledge_graph = format_knowledge_graph({})\n",
        "  df = pd.concat([df, pd.DataFrame([knowledge_graph])], ignore_index=True)\n",
        "\n",
        "  # Answer box\n",
        "  answer_box = None\n",
        "  if 'answer_box' in search_data:\n",
        "    answer_box = format_search_results(\n",
        "        search_data['answer_box'], highlight_field='answer'\n",
        "    )\n",
        "  else:\n",
        "    answer_box = format_search_results({})\n",
        "  df = pd.concat([df, pd.DataFrame([answer_box])], ignore_index=True)\n",
        "\n",
        "  # Sort by date\n",
        "  df['date'] = df['date'].apply(lambda x: format_date(x))\n",
        "  df['datetime'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "  df = df.sort_values(by='datetime', na_position='first')\n",
        "  df.replace({pd.NaT: None}, inplace=True)\n",
        "  df = df.dropna(how='all')\n",
        "\n",
        "  # Select top_k supporting evidences overall\n",
        "  evidences = []\n",
        "\n",
        "  for _, row in df.tail(num_retrieved_evidences).iterrows():\n",
        "    evidences.append(\n",
        "        f\"\"\"\\n\\nsource: {row['source']}\\ndate: {row['date']}\\ntitle: {row['title']}\\nsnippet: {row['snippet']}\\nhighlight: {row['highlight']}\"\"\"\n",
        "    )\n",
        "  # print(type(question),type(search_data),type(reasoning_and_answer))\n",
        "  return (\n",
        "      ''.join(\n",
        "          [\n",
        "              f'\\n\\n\\nquery: {question}',\n",
        "          ]\n",
        "          + evidences #list(search_data)\n",
        "      )\n",
        "      + f'\\n\\nquestion: {question}{reasoning_and_answer}'\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xmQZYfPD3sxL"
      },
      "outputs": [],
      "source": [
        "#@title Demonstration examples\n",
        "\n",
        "\n",
        "demo_questions = [\n",
        "    \"What year is considered Albert Einstein's annus mirabilis?\",\n",
        "    \"Which photographer took the most expensive photograph in the world?\",\n",
        "    \"How many days are left until the 2023 Grammy Awards?\",\n",
        "    \"How many years ago did the Boxing Day Tsunami happen?\",\n",
        "    (\n",
        "        \"When did Amazon become the first publicly traded company to exceed a\"\n",
        "        \" market value of $3 trillion?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "concise_demo_reasonings_and_answers = [\n",
        "    (\n",
        "        \"1905 is considered Albert Einstein's annus mirabilis, his miraculous\"\n",
        "        \" year.\"\n",
        "    ),\n",
        "    (\n",
        "        'The most expensive photograph in the world is \"Le Violon d\\'Ingres\".'\n",
        "        \" The photograph was created by Man Ray.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The 2023 Grammy Awards ceremony was held on February 5, 2023. Thus,\"\n",
        "        \" the ceremony has already taken place.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The disaster occurred on December 26, 2004. Thus, it happened 19 years\"\n",
        "        \" ago.\"\n",
        "    ),\n",
        "    \"Amazon's market capitalization has never exceeded $3 trillion.\",\n",
        "]\n",
        "\n",
        "verbose_demo_reasonings_and_answers = [\n",
        "    (\n",
        "        \"In the year of 1905, Albert Einstein published four groundbreaking\"\n",
        "        \" papers that revolutionized scientific understanding of the universe.\"\n",
        "        \" Thus, scientists call 1905 Albert Einstein's annus mirabilis — his\"\n",
        "        \" year of miracles.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Man Ray's famed \\\"Le Violon d'Ingres\\\" became the most expensive\"\n",
        "        \" photograph ever to sell at auction, sold for $12.4 million on May\"\n",
        "        \" 14th, 2022 at Christie's New York. The black and white image, taken\"\n",
        "        \" in 1924 by the American surrealist artist, transforms a woman's naked\"\n",
        "        \" body into a violin by overlaying the picture of her back with\"\n",
        "        \" f-holes. Thus, Man Ray is the photographer who took the most\"\n",
        "        \" expensive photograph in the world.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The 2023 Grammy Awards, officially known as the 65th Annual Grammy\"\n",
        "        \" Awards ceremony, was held in Los Angeles on February 5, 2023. Thus,\"\n",
        "        \" the event has already taken place.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The Boxing Day Tsunami refers to the 2004 Indian Ocean earthquake and\"\n",
        "        \" tsunami, which is one of the deadliest natural disasters in recorded\"\n",
        "        \" history, killing an estimated 230,000 people across 14 countries. The\"\n",
        "        \" disaster occurred on December 26, 2004, which is 19 years ago.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Amazon's market capitalization hit a peak of roughly $1.9 trillion in\"\n",
        "        \" July 2021. In 2022, Amazon became the first public company ever to\"\n",
        "        \" lose $1 trillion in market value. Thus, Amazon's market value has\"\n",
        "        \" never exceeded $3 trillion. In fact, Apple became the first publicly\"\n",
        "        \" traded U.S. company to exceed a market value of $3 trillion in\"\n",
        "        \" January 2022.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "prefix = (\n",
        "    f\"\\nanswer: As of today {current_date}, the most up-to-date and relevant\"\n",
        "    \" information regarding this query is as follows. \"\n",
        ")\n",
        "\n",
        "concise_demo_reasonings_and_answers = [\n",
        "    prefix + x for x in concise_demo_reasonings_and_answers\n",
        "]\n",
        "verbose_demo_reasonings_and_answers = [\n",
        "    prefix + x for x in verbose_demo_reasonings_and_answers\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MS6fKTK-A8bY"
      },
      "outputs": [],
      "source": [
        "#@title Retrieving search data for demonstration examples\n",
        "\n",
        "\n",
        "demo_search_data = [call_search_engine(q) for q in demo_questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fAcuImw5EP5T"
      },
      "outputs": [],
      "source": [
        "#@title Function calling for FreshPrompt\n",
        "\n",
        "\n",
        "def call_freshprompt(model, question, check_premise=False, verbose=False):\n",
        "  temperature = 0.0\n",
        "  max_tokens = 256\n",
        "  chat_completions = True\n",
        "\n",
        "  if model.startswith('gpt-4'):\n",
        "    num_organic_results = 15\n",
        "    num_related_questions = 3\n",
        "    num_questions_and_answers = 3\n",
        "    num_retrieved_evidences = 15\n",
        "  else:\n",
        "    num_organic_results = 15\n",
        "    num_related_questions = 2\n",
        "    num_questions_and_answers = 2\n",
        "    num_retrieved_evidences = 5\n",
        "\n",
        "  if verbose:\n",
        "    demo_reasonings_and_answers = verbose_demo_reasonings_and_answers\n",
        "  else:\n",
        "    demo_reasonings_and_answers = concise_demo_reasonings_and_answers\n",
        "\n",
        "  # Generate prompts for demo examples\n",
        "  demo_prompts = []\n",
        "  # for q, s, ra in zip(\n",
        "  #     demo_questions, demo_search_data, concise_demo_reasonings_and_answers\n",
        "  # ):\n",
        "  #     demo_prompts.append(\n",
        "  #     freshprompt_format(\n",
        "  #         q,\n",
        "  #         s,\n",
        "  #         ra,\n",
        "  #         num_organic_results,\n",
        "  #         num_related_questions,\n",
        "  #         num_questions_and_answers,\n",
        "  #         num_retrieved_evidences,\n",
        "  #     )\n",
        "  #     )\n",
        "\n",
        "  freshprompt_demo = ''.join(demo_prompts).strip()\n",
        "\n",
        "  if check_premise:\n",
        "    suffix = (\n",
        "        \"\\nPlease check if the question contains a valid premise before\"\n",
        "        \" answering.\\nanswer: \"\n",
        "    )\n",
        "  else:\n",
        "    suffix = \"\\nanswer: \"\n",
        "\n",
        "  freshprompt_question = freshprompt_format(\n",
        "      question,\n",
        "      call_search_engine(question),\n",
        "      suffix,\n",
        "      num_organic_results,\n",
        "      num_related_questions,\n",
        "      num_questions_and_answers,\n",
        "      num_retrieved_evidences,\n",
        "  )\n",
        "  # freshprompt_question = freshprompt_format(\n",
        "  #     question,\n",
        "  #     evidence,\n",
        "  #     suffix,\n",
        "  #     num_organic_results,\n",
        "  #     num_related_questions,\n",
        "  #     num_questions_and_answers,\n",
        "  #     num_retrieved_evidences,\n",
        "  # )\n",
        "\n",
        "  fresh_prompt = freshprompt_demo + freshprompt_question\n",
        "  # fresh_prompt = freshprompt_question\n",
        "\n",
        "  if model == \"gpt-3.5-turbo-1106\":\n",
        "    answer = call_llm_gpt_api(\n",
        "        fresh_prompt, model, temperature, max_tokens, chat_completions\n",
        "    )\n",
        "  elif model == \"mistral-small-latest\":\n",
        "    answer = call_llm_mistral_api(\n",
        "        fresh_prompt, model, temperature\n",
        "    )\n",
        "  elif model == \"meta.llama2-70b-chat-v1\":\n",
        "    answer = call_llm_llama2_api(\n",
        "        fresh_prompt, model, temperature\n",
        "    )\n",
        "  return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1rKbqK_mzSPt"
      },
      "outputs": [],
      "source": [
        "def process_freshqa():\n",
        "    # processing specific to freshqa dataset\n",
        "    df_og = pd.read_csv(\"/content/freshqa.csv\")\n",
        "    new_header = df_og.iloc[1]  # Grab the second row for the new column names\n",
        "    df = df_og.copy().loc[2:][:2] #the final indexing can be used to control how many/which questions to test\n",
        "    df.columns = new_header  # Set the new column names\n",
        "    query_list = df[\"question\"].tolist()\n",
        "    ans_list = df[\"answer_0\"].tolist()\n",
        "    ques_id_list = df[\"id\"].tolist()\n",
        "    effective_year_list = df[\"effective_year\"].tolist()\n",
        "    num_hops_list = df[\"num_hops\"].tolist()\n",
        "    fact_type_list = df[\"fact_type\"].tolist()\n",
        "    premise_list = df[\"false_premise\"].tolist()\n",
        "    return ques_id_list, query_list, ans_list, effective_year_list, num_hops_list, fact_type_list, premise_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ysi6slqytv1j"
      },
      "outputs": [],
      "source": [
        "def process_QAQA():\n",
        "    # processing specific to freshqa dataset\n",
        "    df_og = pd.read_csv(\"/content/QAQA.csv\")\n",
        "    df = df_og.copy()[:2] # slice controls the subset of questions to be tested\n",
        "    query_list = df[\"question\"].tolist()\n",
        "    ans_list = df[\"abstractive_answer\"].tolist()\n",
        "    ques_id_list = df[\"idx\"].tolist()\n",
        "    premise_list = df[\"all_assumptions_valid\"].tolist()\n",
        "    return ques_id_list, query_list, ans_list, premise_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5foDBq3Rw_eO"
      },
      "outputs": [],
      "source": [
        "# Manually upload the dataset csv or through google drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB_Cm9mjG3OD",
        "outputId": "05e77743-6467-460a-e624-9f750397305d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "   ques_id                     question  \\\n",
            "0        0   what did pete burns die of   \n",
            "1        1  what kind of fish is salmon   \n",
            "\n",
            "                                            true_ans all_assumptions_valid  \\\n",
            "0  Pete Burns died following a sudden cardiac arr...             all_valid   \n",
            "1  Salmon is a kind of ray-finned fish in the fam...             all_valid   \n",
            "\n",
            "                                        final_answer  \n",
            "0  Pete Burns, the lead singer of the band Dead o...  \n",
            "1  Salmon is a type of ray-finned fish in the fam...  \n"
          ]
        }
      ],
      "source": [
        "#@title FreshPrompt\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "dataset = \"QAQA\"\n",
        "# model_name = \"meta.llama2-70b-chat-v1\"\n",
        "model_name = \"mistral-small-latest\"\n",
        "# model_name = \"gpt-3.5-turbo-1106\" #@param [\"gpt-4-0125-preview\", \"gpt-4-turbo-preview\", \"gpt-4-1106-preview\", \"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-instruct\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0301\"]\n",
        "check_premise = False  # @param {type:\"boolean\"}\n",
        "# @markdown ### Ask your question here!\n",
        "\n",
        "# question = \"Who is the latest artist confirmed to be performing during the 2024 Grammys telecast?\"  # @param {type:\"string\"}\n",
        "# answer = call_freshprompt(model_name, question, check_premise=check_premise)\n",
        "# button = widgets.Button(description=\"SHOW ANSWER\")\n",
        "# output = widgets.Output()\n",
        "\n",
        "answer_list = []\n",
        "\n",
        "if dataset == \"freshqa\":\n",
        "  ques_id_list, query_list, ans_list, effective_year_list, num_hops_list, fact_type_list, premise_list = process_freshqa()\n",
        "  for ques_id, question in zip(ques_id_list,query_list):\n",
        "    print(ques_id)\n",
        "    answer = call_freshprompt(model_name, question, check_premise=check_premise)\n",
        "    answer_list.append(answer)\n",
        "\n",
        "  qa_data_dict = {\n",
        "                \"ques_id\":ques_id_list,\n",
        "                \"question\":query_list,\n",
        "                \"true_ans\":ans_list,\n",
        "                \"effective_year\":effective_year_list,\n",
        "                \"num_hops\":num_hops_list,\n",
        "                \"fact_type\":fact_type_list,\n",
        "                \"premise\":premise_list,\n",
        "                \"final_answer\":answer_list\n",
        "            }\n",
        "\n",
        "elif dataset == \"QAQA\":\n",
        "  ques_id_list, query_list, ans_list, premise_list = process_QAQA()\n",
        "  for ques_id, question in zip(ques_id_list,query_list):\n",
        "    print(ques_id)\n",
        "    answer = call_freshprompt(model_name, question, check_premise=check_premise)\n",
        "    answer_list.append(answer)\n",
        "\n",
        "  qa_data_dict = {\n",
        "                \"ques_id\":ques_id_list,\n",
        "                \"question\":query_list,\n",
        "                \"true_ans\":ans_list,\n",
        "                \"all_assumptions_valid\": premise_list,\n",
        "                \"final_answer\":answer_list\n",
        "            }\n",
        "\n",
        "df = pd.DataFrame(qa_data_dict)\n",
        "print(df.head())\n",
        "df.to_csv(dataset + model_name + \"_response.csv\",index=False)\n",
        "\n",
        "# def on_button_clicked(b):\n",
        "#   # Display the message within the output widget.\n",
        "#   with output:\n",
        "#     print(f'\\n{answer}')\n",
        "\n",
        "\n",
        "# button.on_click(on_button_clicked)\n",
        "# display(button, output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
