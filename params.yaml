{
    "MODEL": "gpt-3.5-turbo-1106", #default - "gpt-3.5-turbo-1106","gpt-4-turbo-preview","meta.llama2-70b-chat-v1","mistral-small","mistral-large-latest"
    "EVAL_MODEL": "gpt-4-turbo-preview",
    "TEMPERATURE": 0.0, #default - 0.0
    "CANDIDATE_TEMPERATURE": 0.7, #default - 0.3
    "DATASET_NAME": "freshqa", #default - "freshqa", "QAQA"
    "DATASET_PATH": "Data\\", #default - "Data\\"
    "EVIDENCE_BATCH_SAVE_PATH": "Web_Search_Response\\evidence_results_batch_serp_all_freshqa.json", 
    "QUERY_PROMPT_PATH": "Prompts\\minimal_response.txt", #default - "Prompts\\candidate_response.txt"
    "ADVERSARIAL_ATTACK_PROMPT_PATH":"Prompts\\adversarial_attack.txt",
    "AUTO_EVALUATION_PROMPT_PATH": "Prompts\\eval_response_comp.txt",
    "AUTO_EVALUATION_BELIEF_PROMPT_PATH": "Prompts\\eval_response_defend_belief.txt",
    "LLAMA_QUERY_PROMPT_PATH": "Prompts\\minimal_response_meta.txt", #default - "Prompts\\candidate_response_meta.txt"
    "LLAMA_ADVERSARIAL_ATTACK_PROMPT_PATH":"Prompts\\adversarial_attack_meta.txt",
    "LLAMA_AUTO_EVALUATION_PROMPT_PATH": "Prompts\\eval_response_comp_meta.txt",
    "RESULT_SAVE_PATH": "Results\\evidence_test_", #default - "Results\\my_dataframe6.csv"
    "WORKFLOW_RUN_COUNT": 0, #default - 0
    "MAX_CANDIDATE_RESPONSES": 3, #default - 3
    "MAX_WORKFLOW_RUN_COUNT": 1, #default - 1
    "MATCH_CRITERIA": "Half" #default - "Half", takes either "Half" or "Full" as valid input
}