{
    "MODEL": "meta.llama2-70b-chat-v1", #default - "gpt-3.5-turbo-1106","gpt-4-turbo-preview","meta.llama2-70b-chat-v1","mistral-medium-latest","mistral-large-latest"
    "EVAL_MODEL": "gpt-4-turbo-preview", # "gpt-3.5-turbo-0125" mistral-small-latest
    "TEMPERATURE": 0.0, #default - 0.0
    "CANDIDATE_TEMPERATURE": 1.0, #default - 0.3
    "DATASET_NAME": "freshqa", #default - "freshqa", "QAQA"
    "DATASET_PATH": "Data\\", #default - "Data\\"
    "EVIDENCE_BATCH_SAVE_PATH": "Web_Search_Response\\evidence_results_batch_serp_all_freshqa.json", 
    "QUERY_PROMPT_PATH": "Prompts\\minimal_response.txt", #default - "Prompts\\candidate_response.txt"
    "BACKWARD_REASONING_RESP_PROMPT_PATH":"Prompts\\backward_reasoning_resp.txt",
    "BACKWARD_REASONING_QUERY_PROMPT_PATH":"Prompts\\backward_reasoning_query.txt",
    "LLAMA_QUERY_PROMPT_PATH": "Prompts\\minimal_response_meta.txt", #default - "Prompts\\candidate_response_meta.txt"
    "LLAMA_BACKWARD_REASONING_RESP_PROMPT_PATH":"Prompts\\backward_reasoning_resp_meta.txt",
    "LLAMA_BACKWARD_REASONING_QUERY_PROMPT_PATH":"Prompts\\backward_reasoning_query_meta.txt",
    "AUTO_EVALUATION_PROMPT_PATH": "Prompts\\eval_response_comp.txt",
    "AUTO_EVALUATION_QUERY_PROMPT_PATH": "Prompts\\eval_question_comp.txt",
    "LLAMA_AUTO_EVALUATION_QUERY_PROMPT_PATH": "Prompts\\eval_question_comp_meta.txt",
    "RESULT_SAVE_PATH": "Results\\evidence_test_", #default - "Results\\my_dataframe6.csv"
    "WORKFLOW_RUN_COUNT": 0, #default - 0
    "MAX_CANDIDATE_RESPONSES": 3, #default - 3
    "MAX_WORKFLOW_RUN_COUNT": 1, #default - 1
    "MATCH_CRITERIA": "Half" #default - "Half", takes either "Half" or "Full" as valid input
}